{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "otherwise-framing",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The objective is to get a SSD network and apply the needed changes to use it on a Edge TPU. I am sharing this notebook because, even I think I am doing the right steps, in the end it doesn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-content",
   "metadata": {},
   "source": [
    "# Set-up\n",
    "This step is to clone the repository. If you have done it once before, you can omit this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-yellow",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Needed step: This is just for making the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "white-enzyme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import random\n",
    "import io\n",
    "import imageio\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "#from object_detection.utils import colab_utils\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-conservative",
   "metadata": {},
   "source": [
    "## Downloading a friendly model\n",
    "For tflite is recommended to use SSD networks.\n",
    "I have downloaded the following model, it is about \"object detection\". It works with 320x320 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the checkpoint and put it into models/research/object_detection/test_data/\n",
    "\n",
    "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
    "!tar -xf ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
    "!if [ -d \"models/research/object_detection/test_data/checkpoint\" ]; then rm -Rf models/research/object_detection/test_data/checkpoint; fi\n",
    "!mkdir models/research/object_detection/test_data/checkpoint\n",
    "!mv ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liquid-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = '/home/jose/codeWorkspace-2.4.1/tf_2.4.1/models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-german",
   "metadata": {},
   "source": [
    "# Export and run with TFLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-programmer",
   "metadata": {},
   "source": [
    "## Model conversion\n",
    "On this step I convert the pb saved model to .tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tflite_convert --saved_model_dir=/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/saved_model --output_file=/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-bookmark",
   "metadata": {},
   "source": [
    "## Model Quantization (From float to uint8)\n",
    "Once the model is converted, I need to quantize it. The original model picks up a float as tensor input. As I want to run it on an Edge TPU I need the input and output tensors to be uint8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "operating-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating a calibration data set. \n",
    "def representative_dataset_gen():\n",
    "    folder = \"/home/jose/codeWorkspace-2.4.1/tf_2.4.1/images_ssd_mb2_2\"\n",
    "    image_size = 320\n",
    "    raw_test_data = []\n",
    "\n",
    "    files = glob.glob(folder+'/*.jpeg')\n",
    "    for file in files:\n",
    "        image = Image.open(file)\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = image.resize((image_size, image_size))\n",
    "        #Quantizing the image between -1,1;\n",
    "        image = (2.0 / 255.0) * np.float32(image) - 1.0\n",
    "        #image = np.asarray(image).astype(np.float32)\n",
    "        image = image[np.newaxis,:,:,:]\n",
    "        raw_test_data.append(image)\n",
    "\n",
    "    for data in raw_test_data:\n",
    "        yield [data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-bridal",
   "metadata": {},
   "source": [
    "### (DO NOT RUN THIS ONE). It is the above step but with random values\n",
    "If you don't have a dataset, you also can introduce random generated values, as if it was an image. This is the code I used to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "####THIS IS A RANDOM-GENERATED DATASET#### \n",
    "def representative_dataset_gen():\n",
    "    for _ in range(320):\n",
    "      data = np.random.rand(1, 320, 320, 3)\n",
    "      yield [data.astype(np.float32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-offset",
   "metadata": {},
   "source": [
    "## Call for model convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "organized-interference",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n",
      "WARNING:absl:For model outputs containing unsupported operations which cannot be quantized, the `inference_output_type` attribute will default to the original type.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model('/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/saved_model')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "#converter.experimental_new_converter = True\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "converter.allow_custom_ops = True\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-chrome",
   "metadata": {},
   "source": [
    "### WARNING!!!\n",
    "The conversion step returns a warning.\n",
    "\n",
    "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n",
    "\n",
    "WARNING:absl:For model outputs containing unsupported operations which cannot be quantized, the `inference_output_type` attribute will default to the original type.\n",
    "\n",
    "This makes me think conversion is not correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-recognition",
   "metadata": {},
   "source": [
    "# Experimental Area @@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "This area is for new trials. Is not part of the final work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "###IGNORE THIS CELL###  RETURNS THE SAME WARNING AS ABOVE.\n",
    "\n",
    "# Integer Quantization - Input/Output=uint8\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/saved_model')\n",
    "converter.experimental_new_converter = True\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_graph_file = '/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/saved_model/saved_model.pb'\n",
    "input_arrays = [\"normalized_input_image_tensor\"] \n",
    "output_arrays = ['TFLite_Detection_PostProcess',\n",
    "                 'TFLite_Detection_PostProcess:1',\n",
    "                 'TFLite_Detection_PostProcess:2',\n",
    "                 'TFLite_Detection_PostProcess:3']\n",
    "input_shapes = {\"normalized_input_image_tensor\" : [1, 320, 320, 3]}\n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\n",
    "    frozen_graph_file,\n",
    "    input_arrays,output_arrays,\n",
    "    input_shapes) \n",
    "converter.allow_custom_ops = True \n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE] \n",
    "converter.representative_dataset = _representative_dataset_gen \n",
    "tflite_model_quant = converter.convert() \n",
    "with open(tflite_model_quant_file, \"wb\") as tflite_file: \n",
    "    tflite_file.write(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-bottom",
   "metadata": {},
   "source": [
    "# End of Experimental Area @@@@@@@@@@@@@@@@@@"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-hindu",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smaller-professor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tflite convert complete! - /home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/saved_model/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model_full_integer_quant_wImage6.tflite\n"
     ]
    }
   ],
   "source": [
    "with open('/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model_full_integer_quant_wImage6.tflite'.format('/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/saved_model'), 'wb') as w:\n",
    "    w.write(tflite_model)\n",
    "print(\"tflite convert complete! - {}/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model_full_integer_quant_wImage6.tflite\".format('/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/saved_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-connection",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-occupation",
   "metadata": {},
   "source": [
    "## Test 1: Get TensorFlow version\n",
    "I readed that it is recommended to use nightly for this. So in my case, version is 2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-appliance",
   "metadata": {},
   "source": [
    "## Test 2: Get input/output tensor details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model_full_integer_quant_wImage6.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "print(interpreter.get_input_details())\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "print(interpreter.get_output_details())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-priority",
   "metadata": {},
   "source": [
    "### Test 2 Results:\n",
    "\n",
    "I get the following info:\n",
    "\n",
    "[{'name': 'serving_default_input:0', 'index': 0, 'shape': array([  1, 320, 320,   3], dtype=int32), 'shape_signature': array([  1, 320, 320,   3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.007843137718737125, 127), 'quantization_parameters': {'scales': array([0.00784314], dtype=float32), 'zero_points': array([127], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "[{'name': 'StatefulPartitionedCall:31', 'index': 377, 'shape': array([ 1, 10,  4], dtype=int32), 'shape_signature': array([ 1, 10,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:32', 'index': 378, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:33', 'index': 379, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:34', 'index': 380, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
    "\n",
    "So, I think it is not quantizing it right. But in the following step, it seems to work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-history",
   "metadata": {},
   "source": [
    "## Test 3: Real Application. Does the generated TFLite model work?\n",
    "In this step I just want to test if the object detector works. This script acts like a program which picks up a picture, a model and runs the inference. At the end, the script shows the execution time, the objects detected and it draws a red square on the detected objects, showing the photo on the image reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "demanding-exhibition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 320, 320, 3)\n",
      "Warm-up time: 1899.7 ms\n",
      "\n",
      "Inference time: 1876.4 ms\n",
      "\n",
      "104.15686274509804 185.72549019607845 100.3921568627451 205.80392156862746\n",
      "[(104.15686274509804, 100.3921568627451), (104.15686274509804, 205.80392156862746), (185.72549019607845, 205.80392156862746), (185.72549019607845, 100.3921568627451), (104.15686274509804, 100.3921568627451)]\n",
      "0.7215686274509804\n",
      "254.74509803921566 298.6666666666667 242.19607843137254 276.07843137254906\n",
      "[(254.74509803921566, 242.19607843137254), (254.74509803921566, 276.07843137254906), (298.6666666666667, 276.07843137254906), (298.6666666666667, 242.19607843137254), (254.74509803921566, 242.19607843137254)]\n",
      "0.6941176470588235\n",
      "185.72549019607845 227.1372549019608 215.843137254902 249.72549019607843\n",
      "[(185.72549019607845, 215.843137254902), (185.72549019607845, 249.72549019607843), (227.1372549019608, 249.72549019607843), (227.1372549019608, 215.843137254902), (185.72549019607845, 215.843137254902)]\n",
      "0.6313725490196078\n",
      "79.05882352941177 128.0 203.2941176470588 248.47058823529412\n",
      "[(79.05882352941177, 203.2941176470588), (79.05882352941177, 248.47058823529412), (128.0, 248.47058823529412), (128.0, 203.2941176470588), (79.05882352941177, 203.2941176470588)]\n",
      "0.28627450980392155\n",
      "Number of objects found: 4\n",
      "\n",
      "Finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow.lite as tflite\n",
    "\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageColor\n",
    "\n",
    "\n",
    "def load_labels(filename):\n",
    "  with open(filename, 'r') as f:\n",
    "    return [line.strip() for line in f.readlines()]\n",
    "\n",
    "\n",
    "interpreter = tflite.Interpreter('/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model_full_integer_quant_wImage6.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "#print(input_details)\n",
    "output_details = interpreter.get_output_details()\n",
    "#print(output_details)\n",
    "\n",
    "# check the type of the input and output tensors\n",
    "floating_modelI = input_details[0]['dtype'] == np.uint8\n",
    "floating_modelO = output_details[0]['dtype'] == np.uint8\n",
    "#Result: Yes. Input and output are both uint8.\n",
    "\n",
    "\n",
    "# NxHxWxC, H:1, W:2\n",
    "height = input_details[0]['shape'][1]\n",
    "width = input_details[0]['shape'][2]\n",
    "img = Image.open('/home/jose/codeWorkspace-2.4.1/tf_2.4.1/images_ssd_mb2_2/out7.jpeg')#.resize((320, 320))\n",
    "\n",
    "# add N dim\n",
    "input_data = np.expand_dims(img, axis=0)\n",
    "#Test if the array has the expected dimmensions (1, 320, 320, 3)\n",
    "print(input_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "#input_data = (2.0 / 255.0) * np.float32(input_data) - 1.0\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# ignore the 1st invoke\n",
    "startTime = datetime.now()\n",
    "interpreter.invoke()\n",
    "delta = datetime.now() - startTime\n",
    "print(\"Warm-up time:\", '%.1f' % (delta.total_seconds() * 1000), \"ms\\n\")\n",
    "\n",
    "\n",
    "startTime = datetime.now()\n",
    "interpreter.invoke()\n",
    "delta = datetime.now() - startTime\n",
    "print(\"Inference time:\", '%.1f' % (delta.total_seconds() * 1000), \"ms\\n\")\n",
    "\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "num = interpreter.get_tensor(output_details[3]['index'])\n",
    "\n",
    "draw = ImageDraw.Draw(img)\n",
    "im_w, im_h = img.size\n",
    "\n",
    "\n",
    "i = 0\n",
    "while  (scores[0][i]/255) > 0.5:\n",
    "  (ymin, xmin, ymax, xmax) = boxes[0][i]\n",
    "  #(left, right, top, bottom) =(xmin*im_w, xmax*im_w, ymin*im_h, ymax*im_h)\n",
    "  #(left, right, top, bottom) =(xmin, xmax, ymin, ymax)\n",
    "  (left, right, top, bottom) =(xmin/255, xmax/255, ymin/255, ymax/255)\n",
    "  (left, right, top, bottom) =(left*320, right*320, top*320, bottom*320)\n",
    "  #(left, right, top, bottom) =(left+16, right+16, top, bottom)\n",
    "  print(left, right, top, bottom)\n",
    "  #(left, right, top, bottom) =(xmin*1.255, xmax*1.255, ymin*1.255, ymax*1.255)\n",
    "  draw.line([(left,top),(left,bottom),(right,bottom),(right,top),(left,top)],\n",
    "  width = 5, fill = 'red')\n",
    "  print([(left,top),(left,bottom),(right,bottom),(right,top),(left,top)])\n",
    "  #print(im_w)\n",
    "  #print(xmin)\n",
    "  i = i + 1\n",
    "  #Anado el break porque si no me hace un outbounds.\n",
    "  if i == 10:\n",
    "    break\n",
    "  print(scores[0][i]/255)\n",
    "  \n",
    "img.show()\n",
    "#cv_img = cv2.cvtColor(np.asarray(img), cv2.COLOR_RGB2BGR) \n",
    "#cv2.imshow('image',cv_img)\n",
    "#cv2.waitKey(0)\n",
    "\n",
    "print(\"Number of objects found:\",\"%d\\n\" % i)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Finished\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "optional-shoulder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[137,  88, 195, 134],\n",
       "        [102, 140, 128, 172],\n",
       "        [ 14,  18, 115, 163],\n",
       "        [164,  36, 216,  57],\n",
       "        [ 94,  21, 120,  55],\n",
       "        [ 16,  28,  87, 153],\n",
       "        [108,  58, 134,  72],\n",
       "        [ 81,  98, 104, 120],\n",
       "        [158, 146, 228, 237],\n",
       "        [ 16,  16, 233, 233]]], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-floor",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abrir una imagen en el S.O.\n",
    "imgPath = '/home/jose/python-envs/tf_2.4.1/images_ssd_mb2/out1.jpeg'\n",
    "img = Image.open(imgPath)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-president",
   "metadata": {},
   "source": [
    "# Converting the generated model to EdgeTPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "!edgetpu_compiler -s /home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model_full_integer_quant.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-angola",
   "metadata": {},
   "source": [
    "jose@jose-VirtualBox:~/python-envs$ edgetpu_compiler -s /home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model_full_integer_quant.tflite\n",
    "Edge TPU Compiler version 15.0.340273435\n",
    "\n",
    "Model compiled successfully in 1136 ms.\n",
    "\n",
    "Input model: /home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model_full_integer_quant.tflite\n",
    "Input size: 3.70MiB\n",
    "Output model: model_full_integer_quant_edgetpu.tflite\n",
    "Output size: 4.21MiB\n",
    "On-chip memory used for caching model parameters: 3.42MiB\n",
    "On-chip memory remaining for caching model parameters: 4.31MiB\n",
    "Off-chip memory used for streaming uncached model parameters: 0.00B\n",
    "Number of Edge TPU subgraphs: 1\n",
    "Total number of operations: 162\n",
    "Operation log: model_full_integer_quant_edgetpu.log\n",
    "\n",
    "Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\n",
    "Number of operations that will run on Edge TPU: 112\n",
    "Number of operations that will run on CPU: 50\n",
    "\n",
    "Operator                       Count      Status\n",
    "\n",
    "LOGISTIC                       1          Operation is otherwise supported, but not mapped due to some unspecified limitation\n",
    "DEPTHWISE_CONV_2D              14         More than one subgraph is not supported\n",
    "DEPTHWISE_CONV_2D              37         Mapped to Edge TPU\n",
    "QUANTIZE                       1          Mapped to Edge TPU\n",
    "QUANTIZE                       4          Operation is otherwise supported, but not mapped due to some unspecified limitation\n",
    "CONV_2D                        58         Mapped to Edge TPU\n",
    "CONV_2D                        14         More than one subgraph is not supported\n",
    "DEQUANTIZE                     1          Operation is working on an unsupported data type\n",
    "DEQUANTIZE                     1          Operation is otherwise supported, but not mapped due to some unspecified limitation\n",
    "CUSTOM                         1          Operation is working on an unsupported data type\n",
    "ADD                            2          More than one subgraph is not supported\n",
    "ADD                            10         Mapped to Edge TPU\n",
    "CONCATENATION                  1          Operation is otherwise supported, but not mapped due to some unspecified limitation\n",
    "CONCATENATION                  1          More than one subgraph is not supported\n",
    "RESHAPE                        2          Operation is otherwise supported, but not mapped due to some unspecified limitation\n",
    "RESHAPE                        6          Mapped to Edge TPU\n",
    "RESHAPE                        4          More than one subgraph is not supported\n",
    "PACK                           4          Tensor has unsupported rank (up to 3 innermost dimensions mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-schema",
   "metadata": {},
   "source": [
    "The good thing about this is that edgetpu compiler does not throw any errors about incorrect input, or not quantized. If I use this model on a script on the Google Coral, which has an EdgeTPU, it runs and exits without problems, but it fails on the object detection. If with a common tensorflow model and an image it detects 2 objects, now it detects the maximum available by the script, in my case 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-region",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
