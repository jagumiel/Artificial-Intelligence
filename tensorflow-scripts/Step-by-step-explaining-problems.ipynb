{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "organized-replica",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The objective is to get a SSD network and apply the needed changes to use it on a Edge TPU. I am sharing this notebook because, even I think I am doing the right steps, in the end it doesn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-berry",
   "metadata": {},
   "source": [
    "# Set-up\n",
    "This step is to clone the repository. If you have done it once before, you can omit this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Clone the tensorflow models repository if it doesn't already exist\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-mattress",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Needed step: This is just for making the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import random\n",
    "import io\n",
    "import imageio\n",
    "import glob\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "#from object_detection.utils import colab_utils\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-cycling",
   "metadata": {},
   "source": [
    "## Downloading a friendly model\n",
    "For tflite is recommended to use SSD networks.\n",
    "I have downloaded the following model, it is about \"object detection\". It works with 320x320 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the checkpoint and put it into models/research/object_detection/test_data/\n",
    "\n",
    "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
    "!tar -xf ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
    "!if [ -d \"models/research/object_detection/test_data/checkpoint\" ]; then rm -Rf models/research/object_detection/test_data/checkpoint; fi\n",
    "!mkdir models/research/object_detection/test_data/checkpoint\n",
    "!mv ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = '/home/jose/codeWorkspace-2.4.1/tf_2.4.1/models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-subject",
   "metadata": {},
   "source": [
    "# Export and run with TFLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-cabinet",
   "metadata": {},
   "source": [
    "## Model conversion\n",
    "On this step I convert the pb saved model to .tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tflite_convert --saved_model_dir=/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/saved_model --output_file=/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-dealer",
   "metadata": {},
   "source": [
    "## Model Quantization (From float to uint8)\n",
    "Once the model is converted, I need to quantize it. The original model picks up a float as tensor input. As I want to run it on an Edge TPU I need the input and output tensors to be uint8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating a calibration data set. \n",
    "def representative_dataset_gen():\n",
    "    folder = \"/home/jose/codeWorkspace-2.4.1/tf_2.4.1/images_ssd_mb2_2\"\n",
    "    image_size = 320\n",
    "    raw_test_data = []\n",
    "\n",
    "    files = glob.glob(folder+'/*.jpeg')\n",
    "    for file in files:\n",
    "        image = Image.open(file)\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = image.resize((image_size, image_size))\n",
    "        #Quantizing the image between -1,1;\n",
    "        image = (2.0 / 255.0) * np.float32(image) - 1.0\n",
    "        #image = np.asarray(image).astype(np.float32)\n",
    "        image = image[np.newaxis,:,:,:]\n",
    "        raw_test_data.append(image)\n",
    "\n",
    "    for data in raw_test_data:\n",
    "        yield [data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-leisure",
   "metadata": {},
   "source": [
    "### (DO NOT RUN THIS ONE). It is the above step but with random values\n",
    "If you don't have a dataset, you also can introduce random generated values, as if it was an image. This is the code I used to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "####THIS IS A RANDOM-GENERATED DATASET#### \n",
    "def representative_dataset_gen():\n",
    "    for _ in range(320):\n",
    "      data = np.random.rand(1, 320, 320, 3)\n",
    "      yield [data.astype(np.float32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-hammer",
   "metadata": {},
   "source": [
    "## Call for model convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model('/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/saved_model')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "converter.allow_custom_ops = True\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-wrapping",
   "metadata": {},
   "source": [
    "### WARNING!!!\n",
    "The conversion step returns a warning.\n",
    "\n",
    "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n",
    "\n",
    "WARNING:absl:For model outputs containing unsupported operations which cannot be quantized, the `inference_output_type` attribute will default to the original type.\n",
    "\n",
    "This makes me think conversion is not correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-halifax",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model_full_integer_quant.tflite'.format('/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/saved_model'), 'wb') as w:\n",
    "    w.write(tflite_model)\n",
    "print(\"tflite convert complete! - {}/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model_full_integer_quant.tflite\".format('/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/saved_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-miniature",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-canyon",
   "metadata": {},
   "source": [
    "## Test 1: Get TensorFlow version\n",
    "I readed that it is recommended to use nightly for this. So in my case, version is 2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-timothy",
   "metadata": {},
   "source": [
    "## Test 2: Get input/output tensor details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"/home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model_full_integer_quant.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "print(interpreter.get_input_details())\n",
    "print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "print(interpreter.get_output_details())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-activity",
   "metadata": {},
   "source": [
    "### Test 2 Results:\n",
    "\n",
    "I get the following info:\n",
    "\n",
    "[{'name': 'serving_default_input:0', 'index': 0, 'shape': array([  1, 320, 320,   3], dtype=int32), 'shape_signature': array([  1, 320, 320,   3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.007843137718737125, 127), 'quantization_parameters': {'scales': array([0.00784314], dtype=float32), 'zero_points': array([127], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "[{'name': 'StatefulPartitionedCall:31', 'index': 377, 'shape': array([ 1, 10,  4], dtype=int32), 'shape_signature': array([ 1, 10,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:32', 'index': 378, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:33', 'index': 379, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:34', 'index': 380, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
    "\n",
    "So, I think it is not quantizing it right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-cartridge",
   "metadata": {},
   "source": [
    "# Converting the generated model to EdgeTPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "!edgetpu_compiler -s /home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model_full_integer_quant.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-hamburg",
   "metadata": {},
   "source": [
    "jose@jose-VirtualBox:~/python-envs$ edgetpu_compiler -s /home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model_full_integer_quant.tflite\n",
    "Edge TPU Compiler version 15.0.340273435\n",
    "\n",
    "Model compiled successfully in 1136 ms.\n",
    "\n",
    "Input model: /home/jose/codeWorkspace-2.4.1/tf_2.4.1/tflite/model_full_integer_quant.tflite\n",
    "Input size: 3.70MiB\n",
    "Output model: model_full_integer_quant_edgetpu.tflite\n",
    "Output size: 4.21MiB\n",
    "On-chip memory used for caching model parameters: 3.42MiB\n",
    "On-chip memory remaining for caching model parameters: 4.31MiB\n",
    "Off-chip memory used for streaming uncached model parameters: 0.00B\n",
    "Number of Edge TPU subgraphs: 1\n",
    "Total number of operations: 162\n",
    "Operation log: model_full_integer_quant_edgetpu.log\n",
    "\n",
    "Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\n",
    "Number of operations that will run on Edge TPU: 112\n",
    "Number of operations that will run on CPU: 50\n",
    "\n",
    "Operator                       Count      Status\n",
    "\n",
    "LOGISTIC                       1          Operation is otherwise supported, but not mapped due to some unspecified limitation\n",
    "DEPTHWISE_CONV_2D              14         More than one subgraph is not supported\n",
    "DEPTHWISE_CONV_2D              37         Mapped to Edge TPU\n",
    "QUANTIZE                       1          Mapped to Edge TPU\n",
    "QUANTIZE                       4          Operation is otherwise supported, but not mapped due to some unspecified limitation\n",
    "CONV_2D                        58         Mapped to Edge TPU\n",
    "CONV_2D                        14         More than one subgraph is not supported\n",
    "DEQUANTIZE                     1          Operation is working on an unsupported data type\n",
    "DEQUANTIZE                     1          Operation is otherwise supported, but not mapped due to some unspecified limitation\n",
    "CUSTOM                         1          Operation is working on an unsupported data type\n",
    "ADD                            2          More than one subgraph is not supported\n",
    "ADD                            10         Mapped to Edge TPU\n",
    "CONCATENATION                  1          Operation is otherwise supported, but not mapped due to some unspecified limitation\n",
    "CONCATENATION                  1          More than one subgraph is not supported\n",
    "RESHAPE                        2          Operation is otherwise supported, but not mapped due to some unspecified limitation\n",
    "RESHAPE                        6          Mapped to Edge TPU\n",
    "RESHAPE                        4          More than one subgraph is not supported\n",
    "PACK                           4          Tensor has unsupported rank (up to 3 innermost dimensions mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-paste",
   "metadata": {},
   "source": [
    "The good thing about this is that edgetpu compiler does not throw any errors about incorrect input, or not quantized. If I use this model on a script on the Google Coral, which has an EdgeTPU, it runs and exits without problems, but it fails on the object detection. If with a common tensorflow model and an image it detects 2 objects, now it detects the maximum available by the script, in my case 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
