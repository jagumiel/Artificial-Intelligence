{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlUrRaN4w3ct"
      },
      "source": [
        "# Train Your Own Model and Convert It to TFLite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXX-pi1r6NfG"
      },
      "source": [
        "This notebook uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n",
        "\n",
        "Fashion MNIST is intended as a drop-in replacement for the classic [MNIST](http://yann.lecun.com/exdb/mnist/) datasetâ€”often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc.) in a format identical to that of the articles of clothing we'll use here.\n",
        "\n",
        "This uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code.\n",
        "\n",
        "We will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow. Import and load the Fashion MNIST data directly from TensorFlow:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjOAfhgd__Sp"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pfyZKowNAQ4j"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-22 08:48:31.292614: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2021-11-22 08:48:31.296288: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tadPBTEiAprt"
      },
      "source": [
        "# Download Fashion MNIST Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-22 09:38:22.089219: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset 29.45 MiB (download: 29.45 MiB, generated: 36.42 MiB, total: 65.87 MiB) to /home/jose/tensorflow_datasets/fashion_mnist/3.0.1...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-22 09:38:29.925089: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2021-11-22 09:38:29.925117: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2021-11-22 09:38:29.925131: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jose-VirtualBox): /proc/driver/nvidia/version does not exist\n",
            "2021-11-22 09:38:29.925351: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset fashion_mnist downloaded and prepared to /home/jose/tensorflow_datasets/fashion_mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "(train_examples, validation_examples, test_examples), info = tfds.load(\n",
        "    'fashion_mnist',\n",
        "    split=['train[80%:]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    with_info=True, \n",
        "    as_supervised=True, \n",
        ")\n",
        "\n",
        "num_examples = info.splits['train'].num_examples\n",
        "num_classes = info.features['label'].num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ds9gfZKzAnkX"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Split' object has no attribute 'subsplit'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_6666/1536516487.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#splits = tfds.Split.ALL.subsplit(weighted=(80, 10, 10))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fashion_mnist'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Split' object has no attribute 'subsplit'"
          ]
        }
      ],
      "source": [
        "#ESTO NO FUNCIONA. HE PUESTO OTRO CODIGO ARRIBA.\n",
        "\n",
        "#splits = tfds.Split.ALL.subsplit(weighted=(80, 10, 10))\n",
        "splits=tfds.Split.TRAIN.subsplit(weighted=(80, 10, 10))\n",
        "\n",
        "splits, info = tfds.load('fashion_mnist', with_info=True, as_supervised=True, split=splits)\n",
        "\n",
        "(train_examples, validation_examples, test_examples) = splits\n",
        "\n",
        "num_examples = info.splits['train'].num_examples\n",
        "num_classes = info.features['label'].num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-eAv71FRm4JE"
      },
      "outputs": [],
      "source": [
        "class_names = ['T-shirt_top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hXe6jNokqX3_"
      },
      "outputs": [],
      "source": [
        "with open('labels.txt', 'w') as f:\n",
        "  f.write('\\n'.join(class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q0RxpwTmQN-y"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAkuq0V0Aw2X"
      },
      "source": [
        "# Preprocessing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5SIivkunKCC"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nQMIkJf9AvJ4"
      },
      "outputs": [],
      "source": [
        "# Write a function to normalize and resize the images\n",
        "\n",
        "def format_example(image, label):\n",
        "  # Cast image to float32\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  # Resize the image if necessary\n",
        "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "  # Normalize the image in the range [0, 1]\n",
        "  image = image / 255.0\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oEQP743aMv4C"
      },
      "outputs": [],
      "source": [
        "# Set the batch size to 32\n",
        "\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM4HfIJtnNEk"
      },
      "source": [
        "## Create a Dataset from images and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zOL4gSUARFjM"
      },
      "outputs": [],
      "source": [
        "# Prepare the examples by preprocessing the them and then batching them (and optionally prefetching them)\n",
        "\n",
        "# If you wish you can shuffle train set here\n",
        "train_batches = train_examples.cache().shuffle(num_examples//4).batch(BATCH_SIZE).map(format_example).prefetch(1)\n",
        "validation_batches = validation_examples.cache().batch(BATCH_SIZE).map(format_example).prefetch(1)\n",
        "test_batches = test_examples.cache().batch(1).map(format_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-topQaOm_LM"
      },
      "source": [
        "# Building the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4gsYqdIlEFVg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nModel: \"sequential\"\\n_________________________________________________________________\\nLayer (type)                 Output Shape              Param #   \\n=================================================================\\nconv2d (Conv2D)              (None, 26, 26, 16)        160       \\n_________________________________________________________________\\nmax_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \\n_________________________________________________________________\\nconv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \\n_________________________________________________________________\\nflatten (Flatten)            (None, 3872)              0         \\n_________________________________________________________________\\ndense (Dense)                (None, 64)                247872    \\n_________________________________________________________________\\ndense_1 (Dense)              (None, 10)                650       \\n=================================================================\\nTotal params: 253,322\\nTrainable params: 253,322\\nNon-trainable params: 0\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Model: \"sequential\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
        "_________________________________________________________________\n",
        "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
        "_________________________________________________________________\n",
        "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
        "_________________________________________________________________\n",
        "flatten (Flatten)            (None, 3872)              0         \n",
        "_________________________________________________________________\n",
        "dense (Dense)                (None, 64)                247872    \n",
        "_________________________________________________________________\n",
        "dense_1 (Dense)              (None, 10)                650       \n",
        "=================================================================\n",
        "Total params: 253,322\n",
        "Trainable params: 253,322\n",
        "Non-trainable params: 0\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kDqcwksFB1bh"
      },
      "outputs": [],
      "source": [
        "# Build the model shown in the previous cell\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  # Set the input shape to (28, 28, 1), kernel size=3, filters=16 and use ReLU activation,  \n",
        "  tf.keras.layers.Conv2D(16, 3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  # Set the number of filters to 32, kernel size to 3 and use ReLU activation \n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  # Flatten the output layer to 1 dimension\n",
        "  tf.keras.layers.Flatten(),\n",
        "  # Add a fully connected layer with 64 hidden units and ReLU activation\n",
        "  tf.keras.layers.Dense(64, activation='relu'),\n",
        "  # Attach a final softmax classification head\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Set the loss and accuracy metrics\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEMOz-LDnxgD"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JGlNoRtzCP4_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "375/375 [==============================] - 8s 18ms/step - loss: 0.6108 - accuracy: 0.7787 - val_loss: 0.4280 - val_accuracy: 0.8423\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 6s 17ms/step - loss: 0.4088 - accuracy: 0.8560 - val_loss: 0.3783 - val_accuracy: 0.8570\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 6s 17ms/step - loss: 0.3453 - accuracy: 0.8783 - val_loss: 0.3224 - val_accuracy: 0.8825\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 7s 18ms/step - loss: 0.3047 - accuracy: 0.8884 - val_loss: 0.2704 - val_accuracy: 0.9007\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 7s 18ms/step - loss: 0.2658 - accuracy: 0.9004 - val_loss: 0.2616 - val_accuracy: 0.9045\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 6s 17ms/step - loss: 0.2449 - accuracy: 0.9099 - val_loss: 0.2155 - val_accuracy: 0.9198\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 6s 17ms/step - loss: 0.2160 - accuracy: 0.9183 - val_loss: 0.1773 - val_accuracy: 0.9360\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 6s 17ms/step - loss: 0.1926 - accuracy: 0.9294 - val_loss: 0.1765 - val_accuracy: 0.9337\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 6s 17ms/step - loss: 0.1677 - accuracy: 0.9392 - val_loss: 0.1388 - val_accuracy: 0.9522\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 7s 18ms/step - loss: 0.1507 - accuracy: 0.9433 - val_loss: 0.1472 - val_accuracy: 0.9457\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f056485e640>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_batches, \n",
        "          epochs=10,\n",
        "          validation_data=validation_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZT9-7w9n4YO"
      },
      "source": [
        "# Exporting to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9dq78KBkCV2_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-22 09:47:25.875300: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
          ]
        }
      ],
      "source": [
        "export_dir = 'saved_model/1'\n",
        "\n",
        "# Use the tf.saved_model API to export the SavedModel\n",
        "\n",
        "tf.saved_model.save(model, export_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EDGiYrBdE6fl"
      },
      "outputs": [],
      "source": [
        "optimization = tf.lite.Optimize.DEFAULT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RbcS9C00CzGe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-22 09:48:19.403602: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
            "2021-11-22 09:48:19.403625: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
            "2021-11-22 09:48:19.403629: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:372] Ignored change_concat_input_ranges.\n",
            "2021-11-22 09:48:19.404239: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: saved_model/1\n",
            "2021-11-22 09:48:19.405303: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
            "2021-11-22 09:48:19.405315: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: saved_model/1\n",
            "2021-11-22 09:48:19.408617: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
            "2021-11-22 09:48:19.444985: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: saved_model/1\n",
            "2021-11-22 09:48:19.457072: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 52835 microseconds.\n",
            "2021-11-22 09:48:19.467903: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2021-11-22 09:48:19.504205: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor sequential/conv2d/Conv2D because it has fewer than 1024 elements (144).\n",
            "2021-11-22 09:48:19.504226: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor sequential/dense_1/MatMul because it has fewer than 1024 elements (640).\n"
          ]
        }
      ],
      "source": [
        "# Use the TFLiteConverter SavedModel API to initialize the converter\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "\n",
        "# Set the optimzations\n",
        "converter.optimizations = [optimization]\n",
        "\n",
        "# Invoke the converter to finally generate the TFLite model\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "q5PWCDsTC3El"
      },
      "outputs": [],
      "source": [
        "tflite_model_file = 'model_fmnist.tflite'\n",
        "\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR6wFcQ1Fglm"
      },
      "source": [
        "# Test if your model is working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "O3IFOcUEIzQx"
      },
      "outputs": [],
      "source": [
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rKcToCBEC-Bu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-22 09:49:20.727779: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
            "2021-11-22 09:49:20.728334: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
          ]
        }
      ],
      "source": [
        "# Gather results for the randomly sampled test images\n",
        "predictions = []\n",
        "test_labels = []\n",
        "test_images = []\n",
        "\n",
        "for img, label in test_batches.take(50):\n",
        "  interpreter.set_tensor(input_index, img)\n",
        "  interpreter.invoke()\n",
        "  predictions.append(interpreter.get_tensor(output_index))\n",
        "  test_labels.append(label[0])\n",
        "  test_images.append(np.array(img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cellView": "form",
        "id": "kSjTmi05Tyod"
      },
      "outputs": [],
      "source": [
        "#@title Utility functions for plotting\n",
        "# Utilities for plotting\n",
        "\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  \n",
        "  img = np.squeeze(img)\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label.numpy():\n",
        "    color = 'green'\n",
        "  else:\n",
        "    color = 'red'\n",
        "    \n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  predictions_array, true_label = predictions_array[i], true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks(list(range(10)), class_names, rotation='vertical')\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array[0], color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array[0])\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('green')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellView": "form",
        "id": "ZZwg0wFaVXhZ"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x216 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAEkCAYAAAARqOs2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWz0lEQVR4nO3de7TndV3v8ed7BnVURNSZSEkdtLzLZYRjmqsEo/JaiknISTOtY55EZXk8lK0QL8vJ0qyJVJJjoHjBwHNQS81LaHTBmeHmrbM8ISsID4KJikgDvPvj8/nN/PZmz54B5/t9f9vzfKy11+zf9zd73t+957dfv+/3c43MRJI0vlXVJyBJeysDWJKKGMCSVMQAlqQiBrAkFTGAJanIPrfnL69duzbXr19/hwpdddVVd+jrbo8DDzxwyeNb/nXL4LUBHnO/x4xSR9NV+TrXdG3ZsuXazFy3+PjtCuD169ezefPmO3QCJ5100h36uttj48aNSx6PU2Lw2gCbT75jPxutHJWvc01XRFyx1HGbICSpiAEsSUUMYEkqYgBLUhEDWJKKGMCSVMQAlqQiBrAkFTGAJamIASxJRQxgSSpiAEtSEQNYkooYwJJUxACWpCIGsCQVMYAlqYgBLElFDGBJKmIAS1IRA1iSihjAklTEAJakIgawJBUxgCWpiAEsSUUMYEkqYgBLUhEDWJKKGMCSVMQAlqQiBrAkFTGAJamIASxJRQxgSSpiAEtSEQNYkooYwJJUxACWpCIGsCQVMYAlqYgBLElFDGBJKmIAS1IRA1iSihjAklTEAJakIgawJBUxgCWpiAEsSUUMYEkqYgBLUhEDWJKKGMCSVMQAlqQiBrAkFTGAJamIASxJRQxgSSpiAEtSEQNYkooYwJJUxACWpCIGsCQVMYAlqYgBLElFDGBJKmIAS1IRA1iSihjAklTEAJakIgawJBUxgCWpiAEsSUUMYEkqYgBLUhEDWJKKGMCSVMQAlqQiBrAkFTGAJamIASxJRQxgSSpiAEtSEQNYkooYwJJUxACWpCIGsCQVMYAlqYgBLElFDGBJKmIAS1IRA1iSihjAklTEAJakIgawJBUxgCWpiAEsSUUMYEkqYgBLUhEDWJKKGMCSVMQAlqQiBrAkFTGAJamIASxJRQxgSSpiAEtSEQNYkooYwJJUxACWpCIGsCQVMYAlqYgBLElFDGBJKmIAS1IRA1iSihjAklTEAJakIgawJBUxgCWpiAEsSUUMYEkqYgBLUhEDWJKKGMCSVMQAlqQiBrAkFTGAJamIASxJRQxgSSpiAEtSEQNYkooYwJJUxACWpCIGsCQVMYAlqYgBLElFDGBJKmIAS1IRA1iSihjAklTEAJakIgawJBUxgCWpiAEsSUUMYEkqYgBLUhEDWJKKGMCSVMQAlqQiBrAkFTGAJamIASxJRQxgSSpiAEtSEQNYkooYwJJUxACWpCIGsCQVMYAlqYgBLElFDGBJKmIAS1IRA1iSihjAklTEAJakIgawJBUxgCWpiAEsSUUMYEkqYgBLUhEDWJKK7FN9AhrOSSedNHiNjRs3Dl5DWqm8ApakIgawJBUxgCWpiAEsSUUMYEkqYgBLUhEDWJKKGMCSVMQAlqQiBrAkFTGAJamIASxJRQxgSSpiAEtSEQNYkooYwJJUxACWpCIGsCQVMYAlqYgBLElFDGBJKmIAS1IRA1iSihjAklTEAJakIgawJBUxgCWpiAEsSUUMYEkqYgBLUhEDWJKKGMCSVMQAlqQiBrAkFTGAJamIASxJRQxgSSpiAEtSEQNYkooYwJJUxACWpCIGsCQViczc/b8c8Q3giuFOZ4G1wLUj1bL2NOpb29ortfYDM3Pd4oO3K4DHFBGbM/Nwa+899a1t7b2h9jybICSpiAEsSUWmHMCnWXuvq29ta+8NtbebbBuwJK10U74ClqQVba8P4IhYFRGPrz4PScOJiLvszrGxTSqAI+I+EbEpIrZGxJaI+KOIuM+QNTPzVuDUIWssJ5r7V9XX+CLiJ3bn2AB1V0fEK4auM1F/v5vHRrVP9Qks8n7gs8Ax/fHxwAeAnx647qci4hjg3By5UTwzMyL+Enj0mHXnRcQvAh/LzO9ExO8AG4DXZ+bWgev+Xmb+z10d28M1Nyz3/NDfc7eJ9jPe1bE9KjNviYjjgD8css5yIuLEJQ5fD2zJzIsHqPfDwIHAXSPiMCD6U/sBd9vT9W6vSXXCRcQXMvNRi45dlpmDhlNEfAe4O3ALcCPtPykzc78h687VPwP4k8z8/Bj1lqh/aWYeHBFPAF4P/D7wu5n52IHrbs3MDYuOXZqZBw9Y8zPLPJ2ZedSAtR8HPB54OQtDcD/gmZl5yFC1587hD4E70S5sbpgdH+mNh4h4L3A48OF+6GnApcB64IOZ+aY9XO/5wK/0mpvnnvoO8OeZee6erHd7Te0K+BMR8UvA2f3xs4GPD100M+8xdI1deCxwfERcQfulmL0BDBZEi9zS/3wqcFpmfjQiXj9UsYj4DeAlwIMi4tK5p+4BXDBUXYDMPHLIf38X7gzsS/u9m3/NfZv2Wh/Dof3P184dS2CwN55FfgTYkJnfBYiIk4GPAj8JbAH2aABn5hnAGRFxTGaesyf/7T1halfAsyvRW/uhVex4lx7sijQigtbccVBmvq63yd43My8cot4S9R+41PHMHGXdjYj4CHAVcDTtNvhG4MKhrsgi4p7AvYA3AifNPfWdzPzmEDV3ch6PAh4BrJkdy8wzB665Gjg7M4/Z5V9egSLiK8CjM3Nbf3wX4JLMfFhEXJSZhw1Ud3/gd2lBD3A+8NrMvH6IertrUp1wmXmPzFyVmfv0j1X92D0Gbg74U+BxwHP74+8yYsdcD9r7A0f1z7/HuP83z6HdafxsZn4LuDfwP4YqlpnXZ+bXMvO4/v3eSLsK2zciHjBU3Xn9ymtT/ziSduX1jKHrZuYtwP2GrrMzEXFARJweEX/VHz8iIl444imcBfxjRJzc/w8uAN4bEXcHvjRg3dNpzQ7P6R/fBt41YL3dMqkrYICIeAY73qX+JjM/MkLNrZm5Yf4dOCIuGaNNrtc6mdZG9dDMfEhE3I/WHjZ4z3iv/2Dgysy8KSKeCBwMnNnDeMi6TwfeQguka4AHAl/OzEcOWbfXvgw4BLgoMw+JiAOA92Tm0SPUfhutY+iDLGyHHbw9sgfvu4BX9+97H9rPYLRO4Ig4gtYWDnBBZm5e7u/voZoXZ+ahuzo2tkldAUfERuBltHfCLwEvi4g3jlB6W781zH4e69jRDDKGZ9Kuvm4AyMx/ZWEb4dDOAW6JiB+lTdG8P/DeEeq+Hvhx4P9m5kHAk4B/GKEuwI19COLNEbEf7Q1grOGAa4DraO2uT+8fTxup9trMPJv++s7Mm9nRBzCK3tn8PuBDwDUj3fXc2DuZge3D/m4coe6yptYJ9xTg0P6LMRsdcBHwWwPX/WPai+GHIuINtA6R3xm45rx/78PRZm8Adx+xNsCtmXlzRDwL2JSZmyLiohHqbsvM6/pkmFWZ+ZmIeOsIdQE293bBP6N1/nyXkcaFZuYLxqizEzf0sfWz19qP04aBjaLf4b6ZHXc9DwC+Agx91/MbtM64e9I6ub8JPH/gmrs0tQAG2J/2wwG45xgFM/OsiNhCuwIL4Bcy88tj1O7Ojoh3APtHxK8Bv0oLhrFs6+NDn0e7GoM2VGlo34qIfYHPAWdFxDXM3ZIPKTNf0j99e0R8DNgvMy9d7mt+UBHxqsx8U0RsogfgonM6Ycj63YnAecCDI+ICYB3jjcAAeB3trueTmXlYRBwJ/Nehi/Yxxof0ux0y89tD19wdUwvgNwIX9bGaQWsLHvrqd9YGenlmntrbQI+OiKuHbgOdycw/iIijaR0DD6WNwf3rMWp3LwBeDLwhMy+PiIOAd49Q9+eB79PGxR5Pe8N97XJf8INabiJGRGwYeDzs7E198DbPZfwb8FO011kA/8SOoWljKLnr6Ve+J9P7lyJiEqMgptgJd1/giP7wwsz8+gg1L6Z1gq2njUk8D3hkZj5l6Nq9/onABzLzqjHq7eQc7go8IDP/aeS6B7Dw//uagevNJmKsof2fX0ILooOBzZn5uCHrV+t3es+YvdYi4ieBU8fqhIuITwK/QLvYWktrhjgiMwddjyUizgG+AJzRD/0ycEhmPmvIuruUmZP5AD61O8cGqLu1//kq4KX984tG/L5PBr5IuxX/TeCAkX/uT6ddCV3eHx8KnDdC3efQ9hg8AzgTuBx49kjf87m08aizx48C/mKk2g+hdXZ+Avj07GOk2kcAnwd+mNbncglw/zFq9/p3p3X+70Nrgz0BuM8IdS/enWNjf0yiCSIi1tDmZa+NiHuxcL72gSOcQlUbKACZeQpwSkQcDBwLnB8RV2bm0GtgzLwG+C/A3/TzuTgiHjRC3VfTrn6uge2jTz4J/MUItR+amZfNHmTmFyLi4SPUhTb87O3AOykYgRARJ9DC//vAT2fmN0asP2vjvzUiPgpclz0NB3ZjRDwhM/8WHAWx2H+jtQPej9YjPQvgbwN/MkL9qjbQxa4Bvk4bovRDI9bdlpnXtwmB240xDG9VLmxyuI7xhkZeGhHvBN7THx9PW5NgDDdn5ttGqgVARHyYhR1/d6ONfjg9IsjMQSeh9NEWG2kd7K+j/X6tBVZFxPMy82ND1meioyAm1QYcES/NzE3LPH90jts5NYqIeAntdnwd7ero7MwcclbQ4vqnA5+iTQs+hnZbeKfMfPHAdX+f1vb6vn7oWODSHHA1tLnaa2i/lLNJP58F3paZ3x+w5r37pyfQ3mw/BNw0ez4HnIYdET+13POZef5QtXv9zcBv0zpaTwOenJn/EBEPA96XA01BXuI8JjUKYlIBvCuxxOpZe+jfvZylhwWNcRtOn2zygRxgOb7drH83WnPAz/RDH6ctRzlIGPUJHwdk5gV97PFsgPy3gLMy8/8NUbfa3Otsdqux4DU31uutwvyss4j4cmY+fO65i4YO4D72+WTaay2Bv6WNgrhuyLq7MpUmiN0Vu/4rd8jhc5+vAX6Rth7CKDLztyLikIj4zX7oc5l5yRi1+wzAj2ZbJezVY9QE3kofXpht+u25/Vwe3Z97+s6+cE/pbYCvoU1/3v57MHAIHgv8S2Ze3c/h+bQ7jq/1cxlcbwrYBDyctjrbauCGHH7p1fkmrcVtr2NcBVatNb4sr4B3XmtLZj5mpFonAL9ODyLa1OTTlmuO2cP1PwU8K0caExkRn8/MI3by3ODrP/c6XwFeQetz2N4RNuQVUURspXV6fbMP/3o/8FLaqJOHZ+bgEyJ6U8Av0Zq6Dqd1PD8kMwcdbx8Rt7BjqdW70hacoj9ek5mDdnpH0Vrju/Kf7Qp4EIsG56+ivTDH/Nm8CHjsrIc4In6PNi12lACmTcO9LCL+moWLwww1M2v/ZZ6760A1F7s+M/9qpFozq+faeY+lvcmeA5zTx6KPIjO/GhGrs63M9q4+7XzQAM7M1UP++7uhZK3xXZlUAEfEXTLzpmWOfW2g0m+e+/zmXuc5A9VaSrBwONItDNfcspTtzQAj2RwRv5aZC6ZbR8SLaFekY/hM7wQ8l4UdYUPOhFsdEftkWwDnSbS7npmxfhe/FxF3Bi6OiDcBVzOxRbn2pGhrjM/a3V/OjtFNq2kXHq+sObNmUk0QSzUxjNnsUKXPhHs+rVcc2kyhP8/Mt454DusAxhgT2me/fQj4d3YE7uG0Nsln5jizH5famihz2C2JXk2b/HAtbRGaDZmZvVPyjBxh+dFoi///f9rP+hW0UQl/mplfHbq2bmsSARw7Ns57D21R9PmJGG/PzIcNXH/BPHEKVsvvzSCz0QCfy8zBVyOLNvD3ZNrsu1W0n/vNtBXRBl2Todc/kjYDDeCLmfnpoWtW651g9wU+Mdfk9BBg34GvvufPoWTauW5rKgE8v3He51k4EeOMHHih6qp54nPjQpc05LjQXv9E4MnAr2fm5f3Yg4C30XZJLts9dwwR8VTaMojzWxIN/sZTKdoi+H8A3DkzD4qIQ2kXG4PvBqLbmkQAA0TEKuC4zDyroPb2MYrLHRug7s7Ghc425Rx0XGjvfDk6M69ddHwd7QptlMHxFSLi7bTZYEfSpgQ/m7YY0Jjb84yuL8ZzFG23mdnuL+WjAfZWk2l8z7YI+yuKypeslp+ZB2Xmg/qfs89nj8cYlH+nxeHbz+sbjLgWRpHHZ+bzgH/LthbH42iL5Kx025ZoWpvGVdjAIuIJEfGC/vm6vuRAqUmNggA+GRGvpA2Qnh8ONfROuS8GzuxtwdDWTB18nngsszYtDN4jD60T7I48txLM3mC/F20Pvm/S2mZXui9GxHNpIzJ+jDYt+u+Kz2lwMbfvIm1PvDvR+pxG2XdxZ6YWwMf2P//73LEEBrsa7DPBfjnbBoVjzxN/8zLPJe1WcUiHRMRS32sw1y66Qn0k2pZEb2LHSIx31p3OaF5Km/F4E20Njo/TFsdZ6Z4JHAZshbbvYkSMue/ikiYVwNk2ZhzNbEzmrPlh7AU6+vTfMhMYHD+6aDvy/ktmvq4/3he4jLYv2YrudATIzO/RAnisaedTUb3v4pImEcARcVRmfrovzHIbA46CuBDYQNsG6TwKtgkHiIjnLXU8M88co/5e5h30+f99OvBGdkwHPo1x90cbXR/y9kra7i/za2AMfbdVrXrfxSVNIoBpe1R9mqUXYUmGn6U1v034bFTCGHVn5tdFWEObJbWVtkuE9qxJTAcuVLYYfKWs33dxSZMZhlYhIq4E3sKOwJ2f/puZ+Zai89ofeH9m/lxF/ZUsIr4AHNqbnr5CGwP92dlzixdsWWnGXGRKuzaVK2CgrftAWy5uPQtvj4YaHL8a2Jel112ofGe6ASgfIrNCvY+25dO1tJEQn4PtaxSX7pA7kg9H2wBgtMXgKy1aC2L+d3o21n7oZTiXNakr4Ij4GO2XYPESgcuNFvhB6k1inYlYuF3MKuARtF0xTqo7q5VrCtOBq/TJP7AXLQY/ZVML4FFvAcdYiX83z2N+u5ibgSsy88qq89HKMzf64+v98YLF4FfqFfBMRLwwM09fdGxj9UXOZGbCdX/Xd0UYy5NGrHUbEbEmIl5O24HjYcAFmXmB4asBvIM+uaaP/ngjbe2T62mjP1a6YyLi+NmDiDiVcTe+XdIkroAj4jLaLdE+wI8B/0xrn5q10xxceHqDiYgPANto7ZBPpl35vqz2rLQSRcQlmXlI//xU4BuZ+Zr+ePB1T6r1FeDOA/4X8HPAt6bwuzaVTrinVZ9AkUfMFkGJtjPxhcXno5VrCovBj27RioMvAv43cAFwSkTcu7rpZRI/+My8AiAiHgxcmZk3RcQTaVuWr+SxsNtmn/RhUZXnopVtbx39sYWFoyACeGr/GHSZg90xiSaImT4Q/nDaMLS/BP4P8MjMfErhaQ0mdmxUCAs3K5zEEBmtLHvz6I+pmloAb83MDRHxKuDGzNw0lZEKkv5zi4jHc9s5BqV32JNogpizLSKOo22VPZuWvNLXpZU0sIh4N/Bg4GJ2zDFIips4pxbAL6CtzfuGzLy8L5j87l18jSTtyuG0Tu/p3PIzsSaIeRGxwXYpSXtCRHwQOCEzr64+l3mTuAKeGx4z7520pSIl6Qe1FvhSRFzIwjUwSjcjnUQAs2Nd3nmOyZK0p7ym+gSWMpUAXipsTxn9LCStSJl5/vzjvgvOccD5S3/FOKYSwOsi4sTFB2fHqtbllbRyRMRhwHNpa69cDpxTe0bTCeDl1uWVpDukTzQ5rn9cS9txPar3Y5yZxCiIqazLK2lliYhbadOuX5iZX+3H/nkq6x9PZTlKr3wlDeFZwNXAZyLizyLiSUwob6ZyBVy+KpGklatvQ//ztKaIo2gz4D6UmZ8oPa8pBLAkjSUi7kXriDs2M2s3ZTCAJanGVNqAJWmvYwBLUhEDWJKKGMCSVMQAlqQi/wHfjC8gXdvBugAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Visualize the outputs { run: \"auto\" }\n",
        "index = 49 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(index, predictions, test_labels, test_images)\n",
        "plt.show()\n",
        "plot_value_array(index, predictions, test_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "076bo3FMpRDb"
      },
      "source": [
        "# Download TFLite model and assets\n",
        "\n",
        "**NOTE: You might have to run to the cell below twice**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XsPXqPlgZPjE"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  files.download(tflite_model_file)\n",
        "  files.download('labels.txt')\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyBVNwAzH3Oe"
      },
      "source": [
        "# Deploying TFLite model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdfa5L6wH87u"
      },
      "source": [
        "Now once you've the trained TFLite model downloaded, you can ahead and deploy this on an Android/iOS application by placing the model assets in the appropriate location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLY6X8P90L0P"
      },
      "source": [
        "# Prepare the test images for download (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "G3bjzLj10OJv"
      },
      "outputs": [],
      "source": [
        "!mkdir -p test_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pVrBZv1-0Py-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-22 09:50:12.123024: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
            "2021-11-22 09:50:12.123574: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "for index, (image, label) in enumerate(test_batches.take(50)):\n",
        "  image = tf.cast(image * 255.0, tf.uint8)\n",
        "  image = tf.squeeze(image).numpy()\n",
        "  pil_image = Image.fromarray(image)\n",
        "  pil_image.save('test_images/{}_{}.jpg'.format(class_names[label[0]].lower(), index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nX0N0M8u0R2s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'ankle boot_10.jpg'   coat_40.jpg       sandal_19.jpg\t sneaker_43.jpg\n",
            "'ankle boot_32.jpg'   coat_46.jpg       sandal_2.jpg\t trouser_20.jpg\n",
            "'ankle boot_4.jpg'    coat_48.jpg       sandal_39.jpg\t trouser_22.jpg\n",
            " bag_16.jpg\t      dress_12.jpg      shirt_27.jpg\t trouser_35.jpg\n",
            " bag_17.jpg\t      dress_29.jpg      shirt_33.jpg\t trouser_49.jpg\n",
            " bag_23.jpg\t      dress_37.jpg      shirt_5.jpg\t t-shirt_top_15.jpg\n",
            " bag_34.jpg\t      dress_45.jpg      sneaker_13.jpg\t t-shirt_top_18.jpg\n",
            " bag_36.jpg\t      dress_6.jpg       sneaker_24.jpg\t t-shirt_top_1.jpg\n",
            " bag_3.jpg\t      pullover_28.jpg   sneaker_25.jpg\t t-shirt_top_21.jpg\n",
            " bag_7.jpg\t      pullover_44.jpg   sneaker_26.jpg\t t-shirt_top_47.jpg\n",
            " coat_11.jpg\t      pullover_9.jpg    sneaker_38.jpg\t t-shirt_top_8.jpg\n",
            " coat_30.jpg\t      sandal_0.jpg      sneaker_41.jpg\n",
            " coat_31.jpg\t      sandal_14.jpg     sneaker_42.jpg\n"
          ]
        }
      ],
      "source": [
        "!ls test_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LvLht1QM0W8k"
      },
      "outputs": [],
      "source": [
        "!zip -qq fmnist_test_images.zip -r test_images/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FdOq-4sT0X95"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  files.download('fmnist_test_images.zip')\n",
        "except:\n",
        "  pass"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tflite_c03_exercise_convert_model_to_tflite.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
